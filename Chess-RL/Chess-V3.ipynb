{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ea4a8e4-cbcc-4983-b1f3-0af433b3361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.svg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from IPython.display import display, SVG, clear_output\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "add85072-e0c5-4ec1-aa19-23e4a1d07043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c69704e-1823-4f32-bc57-110b7bba345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_tensor(board):\n",
    "    planes = np.zeros((8, 8, 14), dtype=np.float32)\n",
    "    piece_map = {chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,\n",
    "                 chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5}\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            color_offset = 0 if piece.color == chess.WHITE else 6\n",
    "            piece_idx = piece_map[piece.piece_type] + color_offset\n",
    "            row, col = divmod(square, 8)\n",
    "            planes[row, col, piece_idx] = 1\n",
    "    planes[:, :, 12] = 1 if board.has_kingside_castling_rights(chess.WHITE) else 0\n",
    "    planes[:, :, 13] = 1 if board.has_queenside_castling_rights(chess.WHITE) else 0\n",
    "    return planes.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8722d7a-0c5f-4c3a-a44a-e4d85edfb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ChessEnv, self).__init__()\n",
    "        self.board = chess.Board()\n",
    "        self.action_space = spaces.Discrete(4096)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(896,), dtype=np.float32)\n",
    "        self.max_steps = 1000\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board.reset()\n",
    "        self.step_count = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        legal_moves = list(self.board.legal_moves)\n",
    "        if not legal_moves:\n",
    "            return self._get_obs(), -1, True, False, {}\n",
    "        move = self._index_to_move(action, legal_moves)\n",
    "        if move in legal_moves:\n",
    "            self.board.push(move)\n",
    "        self.step_count += 1\n",
    "        reward = 1 if self.board.is_checkmate() else 0.1 if self.board.is_capture(self.board.peek()) else 0\n",
    "        done = self.board.is_game_over()\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "        return self._get_obs(), reward, done, truncated, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        display(SVG(chess.svg.board(self.board, size=400)))\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return board_to_tensor(self.board)\n",
    "\n",
    "    def _index_to_move(self, idx, legal_moves):\n",
    "        return legal_moves[idx % len(legal_moves)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a23ac28-805b-4391-9690-419e1807bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized\n"
     ]
    }
   ],
   "source": [
    "env = ChessEnv()\n",
    "print(\"Environment initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e952752d-04b8-4e17-992d-b791adc87636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessNet(nn.Module):\n",
    "    def __init__(self, action_size=4096):\n",
    "        super(ChessNet, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(896, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # For DQN: Q-values\n",
    "        self.q_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "        # For PPO/A3C: Policy and Value\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_value=False):\n",
    "        features = self.feature_extractor(x)\n",
    "        q_values = self.q_head(features)\n",
    "        if return_value:  # PPO/A3C\n",
    "            policy_logits = self.policy_head(features)\n",
    "            value = self.value_head(features)\n",
    "            return policy_logits, value\n",
    "        return q_values  # DQN\n",
    "\n",
    "    def forward_alpha_zero(self, x):  # AlphaZero: Policy + Value\n",
    "        features = self.feature_extractor(x)\n",
    "        policy_logits = self.policy_head(features)\n",
    "        value = self.value_head(features)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01026bc9-bc58-4d82-b5a1-8b3091be3390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized with ~1451009 parameters\n"
     ]
    }
   ],
   "source": [
    "net = ChessNet().to(device)\n",
    "print(f\"Network initialized with ~{sum(p.numel() for p in net.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8038c7da-ab8b-4c4c-8d9d-e3b775ec6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, lr=0.001, gamma=0.99, buffer_size=10000):\n",
    "        self.env = env\n",
    "        self.policy_net = ChessNet().to(device)\n",
    "        self.target_net = ChessNet().to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.env.action_space.n)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def train(self, batch_size=64, num_timesteps=20000):\n",
    "        for t in range(num_timesteps):\n",
    "            state, _ = self.env.reset()\n",
    "            done = truncated = False\n",
    "            while not (done or truncated):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "                if len(self.replay_buffer) >= batch_size:\n",
    "                    batch = random.sample(self.replay_buffer, batch_size)\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "                    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "                    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "                    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "                    q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "                    next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
    "                    targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "                    loss = nn.MSELoss()(q_values.squeeze(), targets)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            if t % 1000 == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "                print(f\"DQN timestep {t}/{num_timesteps}, epsilon {self.epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3c3fc5b-4dcc-481a-9b0a-1265b24249d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, lr=0.0003, gamma=0.99, clip=0.2):\n",
    "        self.env = env\n",
    "        self.net = ChessNet().to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip = clip\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.net(state_tensor, return_value=True)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def train(self, num_timesteps=20000, rollout_size=2048, epochs=10):\n",
    "        for t in range(0, num_timesteps, rollout_size):\n",
    "            states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "            state, _ = self.env.reset()\n",
    "            for _ in range(rollout_size):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done or truncated)\n",
    "                state = next_state\n",
    "                if done or truncated:\n",
    "                    state, _ = self.env.reset()\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, next_values = self.net(next_states, return_value=True)\n",
    "                returns = rewards + self.gamma * next_values.squeeze() * (1 - dones)\n",
    "\n",
    "            for _ in range(epochs):\n",
    "                logits, values = self.net(states, return_value=True)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                old_probs = probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "                advantages = returns - values.squeeze()\n",
    "\n",
    "                ratio = torch.exp(torch.log(old_probs + 1e-10) - torch.log(old_probs + 1e-10))  \n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip, 1 + self.clip) * advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "                loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print(f\"PPO timestep {t + rollout_size}/{num_timesteps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73e05dd8-0ad7-4d70-a361-505997f733f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C:\n",
    "    def __init__(self, env, lr=0.0001, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.net = ChessNet().to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.net(state_tensor, return_value=True)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def train(self, num_timesteps=20000):\n",
    "        state, _ = self.env.reset()\n",
    "        for t in range(num_timesteps):\n",
    "            states, actions, rewards = [], [], []\n",
    "            done = truncated = False\n",
    "            while not (done or truncated):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                state = next_state\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "\n",
    "            logits, values = self.net(states, return_value=True)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            log_probs = torch.log(probs.gather(1, actions.unsqueeze(1)).squeeze() + 1e-10)\n",
    "            returns = torch.zeros_like(rewards)\n",
    "            R = 0\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                R = rewards[i] + self.gamma * R\n",
    "                returns[i] = R\n",
    "\n",
    "            advantages = returns - values.squeeze()\n",
    "            policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if done or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "\n",
    "            if t % 1000 == 0:\n",
    "                print(f\"A3C timestep {t}/{num_timesteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b557eb5-a331-4468-8fdf-c8a200f8de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "\n",
    "class AlphaZero:\n",
    "    def __init__(self, env, c_puct=1.0, num_simulations=20):\n",
    "        self.env = env\n",
    "        self.net = ChessNet().to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)\n",
    "        self.c_puct = c_puct\n",
    "        self.num_simulations = num_simulations\n",
    "\n",
    "    def mcts(self, root_state):\n",
    "        root = MCTSNode(root_state)\n",
    "        for _ in range(self.num_simulations):\n",
    "            node = root\n",
    "            state = copy.deepcopy(root_state)\n",
    "            env_copy = ChessEnv()\n",
    "            env_copy.board = state\n",
    "\n",
    "            # Selection\n",
    "            while node.children and all(a in node.children for a in list(state.legal_moves)):\n",
    "                node = max(node.children.values(), key=lambda n: n.value / (n.visits + 1) + self.c_puct * np.sqrt(node.visits) / (n.visits + 1))\n",
    "                env_copy.step(node.action)\n",
    "\n",
    "            # Expansion\n",
    "            legal_moves = list(env_copy.board.legal_moves)\n",
    "            if legal_moves and not env_copy.board.is_game_over():\n",
    "                action = random.choice(legal_moves)\n",
    "                next_state, reward, done, _, _ = env_copy.step(env_copy._index_to_move(action.uci(), legal_moves))\n",
    "                if action not in node.children:\n",
    "                    node.children[action] = MCTSNode(env_copy.board, node, action)\n",
    "                node = node.children[action]\n",
    "\n",
    "            # Simulation\n",
    "            state_tensor = torch.tensor(board_to_tensor(state), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits, value = self.net.forward_alpha_zero(state_tensor)\n",
    "            policy = torch.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
    "            value = value.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            while node:\n",
    "                node.visits += 1\n",
    "                node.value += value if node.parent else -value\n",
    "                node = node.parent\n",
    "\n",
    "        return max(root.children.items(), key=lambda x: x[1].visits)[0]\n",
    "\n",
    "    def train(self, num_episodes=150):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            done = truncated = False\n",
    "            while not (done or truncated):\n",
    "                move = self.mcts(self.env.board)\n",
    "                action_idx = list(self.env.board.legal_moves).index(move)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action_idx)\n",
    "                states.append(board_to_tensor(self.env.board))\n",
    "                actions.append(action_idx)\n",
    "                rewards.append(reward)\n",
    "                state = next_state\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "\n",
    "            logits, values = self.net.forward_alpha_zero(states)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            log_probs = torch.log(probs.gather(1, actions.unsqueeze(1)).squeeze() + 1e-10)\n",
    "            returns = torch.zeros_like(rewards)\n",
    "            R = 0\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                R = rewards[i] + 0.99 * R\n",
    "                returns[i] = R\n",
    "\n",
    "            policy_loss = -(log_probs * (returns - values.squeeze()).detach()).mean()\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"AlphaZero episode {episode}/{num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc06be-30e7-4b3f-9196-82afedd3f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DQN training\n",
      "DQN timestep 0/20000, epsilon 0.995\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = DQN(env)\n",
    "print(\"Starting DQN training\")\n",
    "dqn_agent.train(num_timesteps=20000)\n",
    "\n",
    "torch.save(dqn_agent.policy_net.state_dict(), \"dqn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d800857-85fb-4acd-bf21-849cbd22352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = PPO(env)\n",
    "print(\"Starting PPO training\")\n",
    "ppo_agent.train(num_timesteps=20000)\n",
    "\n",
    "torch.save(ppo_agent.net.state_dict(), \"ppo_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826cce8-a6a2-4d0a-929e-027727625413",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3c_agent = A3C(env)\n",
    "print(\"Starting A3C training\")\n",
    "a3c_agent.train(num_timesteps=20000)\n",
    "\n",
    "torch.save(a3c_agent.net.state_dict(), \"a3c_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b856e76-d35a-44b2-9a88-ced60b71f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_zero_agent = AlphaZero(env)\n",
    "print(\"Starting AlphaZero training\")\n",
    "alpha_zero_agent.train(num_episodes=150)\n",
    "\n",
    "torch.save(alpha_zero_agent.net.state_dict(), \"alpha_zero_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80bc8f-b1a9-4cbc-86a2-d5017b94e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_net = ChessNet().to(device)\n",
    "dqn_net.load_state_dict(torch.load(\"dqn_model.pth\"))\n",
    "ppo_net = ChessNet().to(device)\n",
    "ppo_net.load_state_dict(torch.load(\"ppo_model.pth\"))\n",
    "a3c_net = ChessNet().to(device)\n",
    "a3c_net.load_state_dict(torch.load(\"a3c_model.pth\"))\n",
    "alpha_zero_net = ChessNet().to(device)\n",
    "alpha_zero_net.load_state_dict(torch.load(\"alpha_zero_model.pth\"))\n",
    "\n",
    "agents = {\n",
    "    \"DQN\": dqn_agent,\n",
    "    \"PPO\": ppo_agent,\n",
    "    \"A3C\": a3c_agent,\n",
    "    \"AlphaZero\": alpha_zero_agent\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389ace2-d9d1-4ea0-94fa-94f0aee380c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_against_agent(agent_name, human_color=chess.WHITE):\n",
    "    if agent_name not in agents:\n",
    "        print(f\"No agent named {agent_name}\")\n",
    "        return\n",
    "    \n",
    "    agent = agents[agent_name]\n",
    "    board = chess.Board()\n",
    "    env = ChessEnv()\n",
    "    obs, _ = env.reset()\n",
    "    board = env.board.copy()\n",
    "    \n",
    "    print(\"Starting game. Enter moves in UCI format (e.g., 'e2e4'). Type 'quit' to exit.\")\n",
    "    display(SVG(chess.svg.board(board, size=400)))\n",
    "    \n",
    "    move_count = 0\n",
    "    while not board.is_game_over():\n",
    "        move_count += 1\n",
    "        print(f\"\\nMove {move_count}: Turn = {'White' if board.turn == chess.WHITE else 'Black'}\")\n",
    "        \n",
    "        if board.turn == human_color:\n",
    "            move_input = input(\"Your move: \").strip()\n",
    "            print(f\"Received input: {move_input}\")\n",
    "            if move_input.lower() == 'quit':\n",
    "                print(\"Game ended by user.\")\n",
    "                return\n",
    "            try:\n",
    "                move = chess.Move.from_uci(move_input)\n",
    "                legal_moves = list(board.legal_moves)\n",
    "                if move in legal_moves:\n",
    "                    board.push(move)\n",
    "                    env_legal_moves = list(env.board.legal_moves)\n",
    "                    action_idx = env_legal_moves.index(move)\n",
    "                    obs, reward, terminated, truncated, _ = env.step(action_idx)\n",
    "                    board = env.board.copy()\n",
    "                    print(f\"After human move: Reward={reward}, Terminated={terminated}, Truncated={truncated}\")\n",
    "                    if terminated or truncated:\n",
    "                        print(\"Game terminated after human move\")\n",
    "                        break\n",
    "                    clear_output(wait=True)\n",
    "                    display(SVG(chess.svg.board(board, size=400)))\n",
    "                else:\n",
    "                    print(\"Illegal move. Try again.\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Invalid UCI format or error: {e}. Try again.\")\n",
    "        else:\n",
    "            if agent_name == \"AlphaZero\":\n",
    "                move = agent.mcts(board)\n",
    "                action_idx = list(env.board.legal_moves).index(move)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    if agent_name == \"DQN\":\n",
    "                        q_values = dqn_net(state_tensor)\n",
    "                        action_idx = q_values.argmax().item()\n",
    "                    else:  # PPO, A3C\n",
    "                        logits, _ = (ppo_net if agent_name == \"PPO\" else a3c_net)(state_tensor, return_value=True)\n",
    "                        probs = torch.softmax(logits, dim=-1)\n",
    "                        action_idx = torch.multinomial(probs, 1).item()\n",
    "                move = env._index_to_move(action_idx, list(board.legal_moves))\n",
    "            obs, reward, terminated, truncated, _ = env.step(action_idx)\n",
    "            board = env.board.copy()\n",
    "            print(f\"{agent_name}'s move: {move.uci()}, Reward={reward}, Terminated={terminated}, Truncated={truncated}\")\n",
    "            if terminated or truncated:\n",
    "                print(\"Game terminated after agent move\")\n",
    "                break\n",
    "            clear_output(wait=True)\n",
    "            display(SVG(chess.svg.board(board, size=400)))\n",
    "    \n",
    "    print(f\"Game over. Result: {board.result()}\")\n",
    "\n",
    "play_against_agent(\"PPO\", human_color=chess.WHITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182c612-dfdd-4e44-a907-6fab21cd17cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
