{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d79327-bace-4bca-9a00-765295c84142",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea4a8e4-cbcc-4983-b1f3-0af433b3361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.svg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from IPython.display import display, SVG, clear_output\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38a63009-87f0-4af7-9b7f-05223a624259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a04588-d2bf-470e-a055-c83e24a99de3",
   "metadata": {},
   "source": [
    "## Setting up our device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add85072-e0c5-4ec1-aa19-23e4a1d07043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe4baa-3884-40ec-95eb-82a28ca18ba3",
   "metadata": {},
   "source": [
    "# Board encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8722d7a-0c5f-4c3a-a44a-e4d85edfb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert board to 896D tensor (8x8x14: 6 piece types x 2 colors + 2 castling rights).\n",
    "def board_to_tensor(board):\n",
    "    planes = np.zeros((8, 8, 14), dtype=np.float32)\n",
    "    piece_map = {chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,\n",
    "                 chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5}\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            color_offset = 0 if piece.color == chess.WHITE else 6\n",
    "            piece_idx = piece_map[piece.piece_type] + color_offset\n",
    "            row, col = divmod(square, 8)\n",
    "            planes[row, col, piece_idx] = 1\n",
    "    planes[:, :, 12] = 1 if board.has_kingside_castling_rights(chess.WHITE) else 0\n",
    "    planes[:, :, 13] = 1 if board.has_queenside_castling_rights(chess.WHITE) else 0\n",
    "    return planes.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ad38e-b6aa-4a11-bd8e-011104804832",
   "metadata": {},
   "source": [
    "# Setting our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e952752d-04b8-4e17-992d-b791adc87636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv(gym.Env):\n",
    "    # Initialize chess board and Gym spaces.\n",
    "    def __init__(self):\n",
    "        super(ChessEnv, self).__init__()\n",
    "        self.board = chess.Board()\n",
    "        self.action_space = spaces.Discrete(4096)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(896,), dtype=np.float32)\n",
    "        self.max_steps = 1000\n",
    "        self.step_count = 0\n",
    "        self._legal_moves_cache = None\n",
    "\n",
    "    # Reset board to initial position.\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board.reset()\n",
    "        self.step_count = 0\n",
    "        self._legal_moves_cache = None\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    # Execute a move, return observation, reward, done, truncated, info.\n",
    "    def step(self, action):\n",
    "        if self._legal_moves_cache is None:\n",
    "            self._legal_moves_cache = list(self.board.legal_moves)\n",
    "        legal_moves = self._legal_moves_cache\n",
    "\n",
    "        if not legal_moves:\n",
    "            return self._get_obs(), -1, True, False, {}\n",
    "\n",
    "        move = self._index_to_move(action, legal_moves)\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        if move in legal_moves:\n",
    "            self.board.push(move)\n",
    "            self.step_count += 1\n",
    "            if self.board.is_checkmate():\n",
    "                reward = 1\n",
    "                done = True\n",
    "            elif self.board.is_capture(self.board.peek()):\n",
    "                reward = 0.1\n",
    "            done = done or self.board.is_game_over()  # Stalemate, draw, etc.\n",
    "            truncated = self.step_count >= self.max_steps\n",
    "        else:\n",
    "            reward = -0.1  # Penalize invalid moves\n",
    "\n",
    "        self._legal_moves_cache = None  # Clear cache for next step\n",
    "        return self._get_obs(), reward, done, truncated, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        display(SVG(chess.svg.board(self.board, size=400)))\n",
    "\n",
    "    # Get current board state as tensor.\n",
    "    def _get_obs(self):\n",
    "        return board_to_tensor(self.board)\n",
    "\n",
    "    # Map action index to chess move, handle invalid inputs.\n",
    "    def _index_to_move(self, idx, legal_moves):\n",
    "        try:\n",
    "            idx = int(idx)\n",
    "            return legal_moves[idx % len(legal_moves)]\n",
    "        except (TypeError, ValueError) as e:\n",
    "            print(f\"Error in _index_to_move: idx={idx}, type={type(idx)}, error={e}\")\n",
    "            return legal_moves[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8038c7da-ab8b-4c4c-8d9d-e3b775ec6aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chess environment initialized\n"
     ]
    }
   ],
   "source": [
    "env = ChessEnv()\n",
    "print(\"Chess environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bec2f-84e7-4cb2-b048-14071433c9f2",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c3fc5b-4dcc-481a-9b0a-1265b24249d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head network for Q-values, policy logits, and state values.\n",
    "class ChessNet(nn.Module):\n",
    "    def __init__(self, action_size=4096):\n",
    "        super(ChessNet, self).__init__()\n",
    "        # Shared feature extractor: 896D input -> 256D features.\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(896, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # For DQN: Q-values\n",
    "        self.q_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "        # For PPO/A3C: Policy and Value\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_size)\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_value=False):\n",
    "        features = self.feature_extractor(x)\n",
    "        q_values = self.q_head(features)\n",
    "        if return_value:  # PPO/A3C\n",
    "            policy_logits = self.policy_head(features)\n",
    "            value = self.value_head(features)\n",
    "            return policy_logits, value\n",
    "        return q_values  # DQN\n",
    "\n",
    "    # AlphaZero: Policy + Value\n",
    "    def forward_alpha_zero(self, x):  \n",
    "        features = self.feature_extractor(x)\n",
    "        policy_logits = self.policy_head(features)\n",
    "        value = self.value_head(features)\n",
    "        return policy_logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e05dd8-0ad7-4d70-a361-505997f733f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized with ~1451009 parameters\n"
     ]
    }
   ],
   "source": [
    "net = ChessNet().to(device)\n",
    "print(f\"Network initialized with ~{sum(p.numel() for p in net.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04b8ba-1347-4210-b353-4751bd3d4b38",
   "metadata": {},
   "source": [
    "# DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b557eb5-a331-4468-8fdf-c8a200f8de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, lr=0.001, gamma=0.99, buffer_size=10000):\n",
    "        self.env = env\n",
    "        self.policy_net = ChessNet().to(device) # Main Q-network.\n",
    "        self.target_net = ChessNet().to(device) # Frozen target for stability.\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 128  # GPU-friendly batch size.\n",
    "\n",
    "    # Choose action: Epsilon-greedy policy.\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.env.action_space.n)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def train(self, num_timesteps=20000):\n",
    "        t = 0\n",
    "        episode = 0\n",
    "        start_time = time.time()\n",
    "        while t < num_timesteps:\n",
    "            state, _ = self.env.reset()\n",
    "            done = truncated = False\n",
    "            episode_steps = 0\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # Play one episode\n",
    "            while not (done or truncated) and t < num_timesteps:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "                episode_steps += 1\n",
    "                t += 1\n",
    "\n",
    "                # Update network if enough experiences.\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "                    states = torch.tensor([s for s, _, _, _, _ in batch], dtype=torch.float32).to(device)\n",
    "                    actions = torch.tensor([a for _, a, _, _, _ in batch], dtype=torch.long).to(device)\n",
    "                    rewards = torch.tensor([r for _, _, r, _, _ in batch], dtype=torch.float32).to(device)\n",
    "                    next_states = torch.tensor([ns for _, _, _, ns, _ in batch], dtype=torch.float32).to(device)\n",
    "                    dones = torch.tensor([d for _, _, _, _, d in batch], dtype=torch.float32).to(device)\n",
    "\n",
    "                    q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "                    with torch.no_grad():\n",
    "                        next_q_values = self.target_net(next_states).max(1)[0]\n",
    "                    targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "                    loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                if t % 1000 == 0:\n",
    "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"DQN timestep {t}/{num_timesteps}, epsilon {self.epsilon:.3f}, time {elapsed:.1f}s\")\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            episode += 1\n",
    "            episode_time = time.time() - episode_start\n",
    "            print(f\"DQN episode {episode}, steps {episode_steps}, time {episode_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36cfccf-a53c-4b29-aecb-ff07ef09f186",
   "metadata": {},
   "source": [
    "# PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6602a26e-530f-431e-a8cb-17a5850f67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, lr=0.0003, gamma=0.99, clip=0.2):\n",
    "        self.env = env\n",
    "        self.net = ChessNet().to(device) # Policy and value network.\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip = clip\n",
    "        self.batch_size = 128\n",
    "\n",
    "    # Sample action from policy, return probability.\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.net(state_tensor, return_value=True)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action, probs[0, action].item()\n",
    "\n",
    "    def train(self, num_timesteps=20000, rollout_size=1024, epochs=5):\n",
    "        t = 0\n",
    "        episode = 0\n",
    "        start_time = time.time()\n",
    "        while t < num_timesteps:\n",
    "            states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "            state, _ = self.env.reset()\n",
    "            episode_steps = 0\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # Collect rollout\n",
    "            for _ in range(rollout_size):\n",
    "                if t >= num_timesteps:\n",
    "                    break\n",
    "                action, prob = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    _, value = self.net(state_tensor, return_value=True)\n",
    "                \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(np.log(prob + 1e-10))\n",
    "                values.append(value.item())\n",
    "                state = next_state\n",
    "                episode_steps += 1\n",
    "                t += 1\n",
    "\n",
    "                if done or truncated:\n",
    "                    state, _ = self.env.reset()\n",
    "                    episode += 1\n",
    "                    episode_time = time.time() - episode_start\n",
    "                    print(f\"PPO episode {episode}, steps {episode_steps}, time {episode_time:.1f}s\")\n",
    "                    episode_steps = 0\n",
    "                    episode_start = time.time()\n",
    "\n",
    "            # Process rollout\n",
    "            states_tensor = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions_tensor = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            old_log_probs = torch.tensor(log_probs, dtype=torch.float32).to(device)\n",
    "            old_values = torch.tensor(values, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Compute returns\n",
    "            returns = torch.zeros_like(rewards_tensor)\n",
    "            R = 0\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                R = rewards[i] + self.gamma * R\n",
    "                returns[i] = R\n",
    "\n",
    "            # Update policy\n",
    "            for _ in range(epochs):\n",
    "                logits, values = self.net(states_tensor, return_value=True)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                new_log_probs = torch.log(probs.gather(1, actions_tensor.unsqueeze(1)).squeeze() + 1e-10)\n",
    "                advantages = returns - old_values\n",
    "\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip, 1 + self.clip) * advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "                loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if t % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"PPO timestep {t}/{num_timesteps}, time {elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4e0f3-6d74-4d38-8eb1-2b81fd29408e",
   "metadata": {},
   "source": [
    "# A3C Algorithm (Single-Threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6327ba4-e438-4c3f-815b-ee8fcbed7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C:\n",
    "    def __init__(self, env, lr=0.0001, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.net = ChessNet().to(device) # Shared actor-critic network.\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = 128\n",
    "\n",
    "    # Sample action, return probability.\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.net(state_tensor, return_value=True)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action, probs[0, action].item()\n",
    "\n",
    "    def train(self, num_timesteps=20000):\n",
    "        t = 0\n",
    "        episode = 0\n",
    "        start_time = time.time()\n",
    "        while t < num_timesteps:\n",
    "            states, actions, rewards, log_probs = [], [], [], []\n",
    "            state, _ = self.env.reset()\n",
    "            done = truncated = False\n",
    "            episode_steps = 0\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # Collect experience batch.\n",
    "            while not (done or truncated) and t < num_timesteps:\n",
    "                action, prob = self.select_action(state)\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(np.log(prob + 1e-10))\n",
    "                state = next_state\n",
    "                episode_steps += 1\n",
    "                t += 1\n",
    "\n",
    "                # Update network per batch.\n",
    "                if len(states) >= self.batch_size or done or truncated:\n",
    "                    states_tensor = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                    actions_tensor = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "                    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "                    log_probs_tensor = torch.tensor(log_probs, dtype=torch.float32).to(device)\n",
    "\n",
    "                    logits, values = self.net(states_tensor, return_value=True)\n",
    "                    returns = torch.zeros_like(rewards_tensor)\n",
    "                    R = 0\n",
    "                    for i in reversed(range(len(rewards))):\n",
    "                        R = rewards[i] + self.gamma * R\n",
    "                        returns[i] = R\n",
    "\n",
    "                    advantages = returns - values.squeeze()\n",
    "                    policy_loss = -(log_probs_tensor * advantages.detach()).mean()\n",
    "                    value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "                    loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "                if done or truncated:\n",
    "                    state, _ = self.env.reset()\n",
    "                    episode += 1\n",
    "                    episode_time = time.time() - episode_start\n",
    "                    print(f\"A3C episode {episode}, steps {episode_steps}, time {episode_time:.1f}s\")\n",
    "                    episode_steps = 0\n",
    "                    episode_start = time.time()\n",
    "\n",
    "            if t % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"A3C timestep {t}/{num_timesteps}, time {elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853fc97-1f15-4183-9f48-fe38af65d8a3",
   "metadata": {},
   "source": [
    "# AlphaZero (MCTS-guided policy/value learning) -- Simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f3b86fb-2a2b-4e2c-8538-7eff0d3372fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state # Chess board state.\n",
    "        self.parent = parent\n",
    "        self.action = action # Move leading to this node.\n",
    "        self.children = {} # Child nodes (moves).\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "\n",
    "class AlphaZero:\n",
    "    def __init__(self, env, c_puct=1.0, num_simulations=10):\n",
    "        self.env = env\n",
    "        self.net = ChessNet().to(device) # Policy/value network.\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)\n",
    "        self.c_puct = c_puct # Exploration constant.\n",
    "        self.num_simulations = num_simulations # MCTS iterations.\n",
    "\n",
    "    # Run MCTS to select best move.\n",
    "    def mcts(self, root_state):\n",
    "        root = MCTSNode(root_state)\n",
    "        for _ in range(self.num_simulations):\n",
    "            node = root\n",
    "            state = copy.deepcopy(root_state)\n",
    "            env_copy = ChessEnv()\n",
    "            env_copy.board = state\n",
    "\n",
    "            # Selection: Choose best child by UCB.\n",
    "            legal_moves = list(state.legal_moves)\n",
    "            while node.children and all(a in node.children for a in legal_moves):\n",
    "                node = max(node.children.values(), key=lambda n: n.value / (n.visits + 1) + self.c_puct * np.sqrt(node.visits) / (n.visits + 1))\n",
    "                action_idx = list(state.legal_moves).index(node.action)\n",
    "                env_copy.step(action_idx)  # Pass integer index\n",
    "                state = env_copy.board\n",
    "\n",
    "            # Expansion: Add new child node.\n",
    "            legal_moves = list(env_copy.board.legal_moves)\n",
    "            if legal_moves and not env_copy.board.is_game_over():\n",
    "                action = random.choice(legal_moves)\n",
    "                action_idx = list(env_copy.board.legal_moves).index(action)\n",
    "                next_state, reward, done, _, _ = env_copy.step(action_idx)\n",
    "                if action not in node.children:\n",
    "                    node.children[action] = MCTSNode(env_copy.board, node, action)\n",
    "                node = node.children[action]\n",
    "\n",
    "            # Simulation: Estimate value with network.\n",
    "            state_tensor = torch.tensor(board_to_tensor(state), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits, value = self.net.forward_alpha_zero(state_tensor)\n",
    "            value = value.item()\n",
    "\n",
    "            # Backpropagation: Update node stats.\n",
    "            while node:\n",
    "                node.visits += 1\n",
    "                node.value += value if node.parent else -value\n",
    "                node = node.parent\n",
    "\n",
    "        if not root.children:\n",
    "            print(\"Warning: No MCTS children generated\")\n",
    "            return random.choice(legal_moves) if legal_moves else None\n",
    "        return max(root.children.items(), key=lambda x: x[1].visits)[0]\n",
    "\n",
    "    def train(self, num_episodes=150):\n",
    "        episode = 0\n",
    "        start_time = time.time()\n",
    "        while episode < num_episodes:\n",
    "            states, actions, rewards = [], [], []\n",
    "            state, _ = self.env.reset()\n",
    "            done = truncated = False\n",
    "            episode_steps = 0\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # Play one episode with MCTS.\n",
    "            while not (done or truncated):\n",
    "                move = self.mcts(self.env.board)\n",
    "                if move is None:\n",
    "                    print(\"MCTS returned None; skipping episode\")\n",
    "                    break\n",
    "                try:\n",
    "                    action_idx = list(self.env.board.legal_moves).index(move)\n",
    "                except ValueError:\n",
    "                    print(f\"Invalid move {move}; skipping\")\n",
    "                    break\n",
    "\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action_idx)\n",
    "                states.append(board_to_tensor(self.env.board))\n",
    "                actions.append(action_idx)\n",
    "                rewards.append(reward)\n",
    "                state = next_state\n",
    "                episode_steps += 1\n",
    "\n",
    "                # Update network with episode data.\n",
    "                if done or truncated:\n",
    "                    states_tensor = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                    actions_tensor = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "                    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "\n",
    "                    logits, values = self.net.forward_alpha_zero(states_tensor)\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    log_probs = torch.log(probs.gather(1, actions_tensor.unsqueeze(1)).squeeze() + 1e-10)\n",
    "                    returns = torch.zeros_like(rewards_tensor)\n",
    "                    R = 0\n",
    "                    for i in reversed(range(len(rewards))):\n",
    "                        R = rewards[i] + 0.99 * R\n",
    "                        returns[i] = R\n",
    "\n",
    "                    policy_loss = -(log_probs * (returns - values.squeeze()).detach()).mean()\n",
    "                    value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "                    loss = policy_loss + value_loss\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    episode += 1\n",
    "                    episode_time = time.time() - episode_start\n",
    "                    print(f\"AlphaZero episode {episode}/{num_episodes}, steps {episode_steps}, time {episode_time:.1f}s\")\n",
    "                    episode_steps = 0\n",
    "                    episode_start = time.time()\n",
    "\n",
    "                    if episode >= num_episodes:\n",
    "                        break\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"AlphaZero total time {elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82fe457-cc1f-452e-b4a0-e5fb1e0c49e6",
   "metadata": {},
   "source": [
    "# Training and Saving agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6c3ac14-4f15-4a15-b531-c12001bc52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DQN training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\top\\AppData\\Local\\Temp\\ipykernel_24004\\3051387646.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.tensor([s for s, _, _, _, _ in batch], dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN episode 1, steps 308, time 4.5s\n",
      "DQN episode 2, steps 333, time 5.9s\n",
      "DQN timestep 1000/20000, epsilon 0.990, time 16.6s\n",
      "DQN episode 3, steps 643, time 11.1s\n",
      "DQN episode 4, steps 396, time 6.9s\n",
      "DQN timestep 2000/20000, epsilon 0.980, time 33.8s\n",
      "DQN episode 5, steps 339, time 5.8s\n",
      "DQN episode 6, steps 317, time 5.2s\n",
      "DQN episode 7, steps 245, time 4.2s\n",
      "DQN timestep 3000/20000, epsilon 0.966, time 50.9s\n",
      "DQN episode 8, steps 426, time 7.5s\n",
      "DQN episode 9, steps 151, time 2.6s\n",
      "DQN episode 10, steps 359, time 6.3s\n",
      "DQN episode 11, steps 234, time 4.1s\n",
      "DQN timestep 4000/20000, epsilon 0.946, time 68.3s\n",
      "DQN episode 12, steps 537, time 9.5s\n",
      "DQN episode 13, steps 472, time 8.2s\n",
      "DQN episode 14, steps 89, time 1.5s\n",
      "DQN timestep 5000/20000, epsilon 0.932, time 85.8s\n",
      "DQN episode 15, steps 358, time 6.3s\n",
      "DQN episode 16, steps 370, time 6.5s\n",
      "DQN timestep 6000/20000, epsilon 0.923, time 103.5s\n",
      "DQN episode 17, steps 555, time 9.8s\n",
      "DQN episode 18, steps 377, time 6.7s\n",
      "DQN episode 19, steps 263, time 4.6s\n",
      "DQN timestep 7000/20000, epsilon 0.909, time 121.0s\n",
      "DQN episode 20, steps 268, time 4.6s\n",
      "DQN episode 21, steps 343, time 5.8s\n",
      "DQN episode 22, steps 401, time 7.0s\n",
      "DQN timestep 8000/20000, epsilon 0.896, time 138.3s\n",
      "DQN episode 23, steps 364, time 6.5s\n",
      "DQN episode 24, steps 109, time 1.9s\n",
      "DQN episode 25, steps 292, time 5.2s\n",
      "DQN timestep 9000/20000, epsilon 0.882, time 156.1s\n",
      "DQN episode 26, steps 546, time 9.7s\n",
      "DQN episode 27, steps 454, time 8.1s\n",
      "DQN episode 28, steps 349, time 6.2s\n",
      "DQN timestep 10000/20000, epsilon 0.869, time 174.0s\n",
      "DQN episode 29, steps 265, time 4.8s\n",
      "DQN episode 30, steps 421, time 7.6s\n",
      "DQN episode 31, steps 258, time 4.6s\n",
      "DQN timestep 11000/20000, epsilon 0.856, time 191.9s\n",
      "DQN episode 32, steps 396, time 6.9s\n",
      "DQN episode 33, steps 365, time 6.4s\n",
      "DQN episode 34, steps 153, time 2.7s\n",
      "DQN timestep 12000/20000, epsilon 0.843, time 209.4s\n",
      "DQN episode 35, steps 379, time 6.5s\n",
      "DQN episode 36, steps 302, time 5.3s\n",
      "DQN episode 37, steps 480, time 8.3s\n",
      "DQN timestep 13000/20000, epsilon 0.831, time 226.7s\n",
      "DQN episode 38, steps 268, time 4.6s\n",
      "DQN episode 39, steps 514, time 8.9s\n",
      "DQN timestep 14000/20000, epsilon 0.822, time 244.2s\n",
      "DQN episode 40, steps 408, time 7.3s\n",
      "DQN episode 41, steps 128, time 2.3s\n",
      "DQN episode 42, steps 334, time 6.0s\n",
      "DQN timestep 15000/20000, epsilon 0.810, time 262.2s\n",
      "DQN episode 43, steps 548, time 9.8s\n",
      "DQN episode 44, steps 443, time 7.8s\n",
      "DQN episode 45, steps 164, time 2.9s\n",
      "DQN timestep 16000/20000, epsilon 0.798, time 279.9s\n",
      "DQN episode 46, steps 324, time 5.8s\n",
      "DQN episode 47, steps 314, time 5.6s\n",
      "DQN episode 48, steps 372, time 6.7s\n",
      "DQN episode 49, steps 101, time 1.8s\n",
      "DQN timestep 17000/20000, epsilon 0.782, time 297.9s\n",
      "DQN episode 50, steps 308, time 5.5s\n",
      "DQN episode 51, steps 371, time 6.7s\n",
      "DQN timestep 18000/20000, epsilon 0.774, time 315.9s\n",
      "DQN episode 52, steps 543, time 9.7s\n",
      "DQN episode 53, steps 421, time 7.3s\n",
      "DQN episode 54, steps 77, time 1.4s\n",
      "DQN episode 55, steps 392, time 6.9s\n",
      "DQN timestep 19000/20000, epsilon 0.759, time 333.4s\n",
      "DQN episode 56, steps 451, time 8.1s\n",
      "DQN timestep 20000/20000, epsilon 0.755, time 351.3s\n",
      "DQN episode 57, steps 602, time 10.7s\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = DQN(env)\n",
    "\n",
    "print(\"Starting DQN training\")\n",
    "dqn_agent.train(num_timesteps=20000)\n",
    "torch.save(dqn_agent.policy_net.state_dict(), \"dqn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8239d2c3-7baf-4c7a-a9e9-ddd779cd2418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training\n",
      "PPO episode 1, steps 485, time 0.5s\n",
      "PPO episode 2, steps 518, time 0.4s\n",
      "PPO episode 3, steps 107, time 0.1s\n",
      "PPO episode 4, steps 460, time 0.4s\n",
      "PPO episode 5, steps 343, time 0.3s\n",
      "PPO episode 6, steps 292, time 0.3s\n",
      "PPO episode 7, steps 483, time 0.4s\n",
      "PPO episode 8, steps 586, time 0.5s\n",
      "PPO episode 9, steps 428, time 0.4s\n",
      "PPO episode 10, steps 264, time 0.2s\n",
      "PPO episode 11, steps 357, time 0.3s\n",
      "PPO episode 12, steps 253, time 0.2s\n",
      "PPO episode 13, steps 536, time 0.5s\n",
      "PPO episode 14, steps 502, time 0.5s\n",
      "PPO episode 15, steps 179, time 0.2s\n",
      "PPO episode 16, steps 227, time 0.2s\n",
      "PPO episode 17, steps 59, time 0.1s\n",
      "PPO episode 18, steps 149, time 0.1s\n",
      "PPO episode 19, steps 259, time 0.2s\n",
      "PPO episode 20, steps 377, time 0.3s\n",
      "PPO episode 21, steps 461, time 0.4s\n",
      "PPO episode 22, steps 121, time 0.1s\n",
      "PPO episode 23, steps 403, time 0.3s\n",
      "PPO episode 24, steps 440, time 0.4s\n",
      "PPO episode 25, steps 245, time 0.2s\n",
      "PPO episode 26, steps 306, time 0.3s\n",
      "PPO episode 27, steps 332, time 0.3s\n",
      "PPO episode 28, steps 281, time 0.3s\n",
      "PPO episode 29, steps 164, time 0.2s\n",
      "PPO episode 30, steps 477, time 0.4s\n",
      "PPO episode 31, steps 72, time 0.1s\n",
      "PPO episode 32, steps 338, time 0.3s\n",
      "PPO episode 33, steps 399, time 0.4s\n",
      "PPO episode 34, steps 315, time 0.3s\n",
      "PPO episode 35, steps 304, time 0.3s\n",
      "PPO episode 36, steps 380, time 0.3s\n",
      "PPO episode 37, steps 478, time 0.4s\n",
      "PPO episode 38, steps 459, time 0.4s\n",
      "PPO episode 39, steps 467, time 0.4s\n",
      "PPO episode 40, steps 307, time 0.3s\n",
      "PPO episode 41, steps 89, time 0.1s\n",
      "PPO episode 42, steps 120, time 0.1s\n",
      "PPO episode 43, steps 553, time 0.5s\n",
      "PPO episode 44, steps 217, time 0.2s\n",
      "PPO episode 45, steps 603, time 0.5s\n",
      "PPO episode 46, steps 427, time 0.5s\n",
      "PPO episode 47, steps 365, time 0.5s\n",
      "PPO episode 48, steps 444, time 0.4s\n",
      "PPO timestep 20000/20000, time 19.6s\n"
     ]
    }
   ],
   "source": [
    "ppo_agent = PPO(env)\n",
    "\n",
    "print(\"Starting PPO training\")\n",
    "ppo_agent.train(num_timesteps=20000)\n",
    "torch.save(ppo_agent.net.state_dict(), \"ppo_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "661a2a3c-e356-4329-af4d-2ae7ea516d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting A3C training\n",
      "A3C episode 1, steps 533, time 0.7s\n",
      "A3C episode 2, steps 237, time 0.2s\n",
      "A3C episode 3, steps 291, time 0.3s\n",
      "A3C episode 4, steps 370, time 0.3s\n",
      "A3C episode 5, steps 273, time 0.2s\n",
      "A3C episode 6, steps 457, time 0.4s\n",
      "A3C episode 7, steps 456, time 0.4s\n",
      "A3C episode 8, steps 266, time 0.3s\n",
      "A3C episode 9, steps 359, time 0.3s\n",
      "A3C episode 10, steps 498, time 0.4s\n",
      "A3C episode 11, steps 575, time 0.5s\n",
      "A3C episode 12, steps 496, time 0.4s\n",
      "A3C episode 13, steps 341, time 0.3s\n",
      "A3C episode 14, steps 348, time 0.3s\n",
      "A3C episode 15, steps 239, time 0.2s\n",
      "A3C episode 16, steps 362, time 0.3s\n",
      "A3C episode 17, steps 411, time 0.3s\n",
      "A3C episode 18, steps 389, time 0.3s\n",
      "A3C episode 19, steps 459, time 0.4s\n",
      "A3C episode 20, steps 396, time 0.3s\n",
      "A3C episode 21, steps 357, time 0.3s\n",
      "A3C episode 22, steps 507, time 0.4s\n",
      "A3C episode 23, steps 493, time 0.4s\n",
      "A3C episode 24, steps 466, time 0.3s\n",
      "A3C episode 25, steps 286, time 0.2s\n",
      "A3C episode 26, steps 85, time 0.1s\n",
      "A3C episode 27, steps 349, time 0.2s\n",
      "A3C episode 28, steps 161, time 0.1s\n",
      "A3C episode 29, steps 362, time 0.3s\n",
      "A3C episode 30, steps 380, time 0.3s\n",
      "A3C episode 31, steps 414, time 0.3s\n",
      "A3C episode 32, steps 454, time 0.3s\n",
      "A3C episode 33, steps 199, time 0.1s\n",
      "A3C episode 34, steps 226, time 0.2s\n",
      "A3C episode 35, steps 354, time 0.3s\n",
      "A3C episode 36, steps 498, time 0.4s\n",
      "A3C episode 37, steps 367, time 0.3s\n",
      "A3C episode 38, steps 320, time 0.3s\n",
      "A3C episode 39, steps 496, time 0.4s\n",
      "A3C episode 40, steps 184, time 0.2s\n",
      "A3C episode 41, steps 156, time 0.1s\n",
      "A3C episode 42, steps 516, time 0.4s\n",
      "A3C episode 43, steps 556, time 0.4s\n",
      "A3C episode 44, steps 352, time 0.4s\n",
      "A3C episode 45, steps 334, time 0.3s\n",
      "A3C episode 46, steps 350, time 0.3s\n",
      "A3C episode 47, steps 319, time 0.3s\n",
      "A3C episode 48, steps 194, time 0.2s\n",
      "A3C episode 49, steps 394, time 0.3s\n",
      "A3C episode 50, steps 518, time 0.4s\n",
      "A3C episode 51, steps 476, time 0.4s\n",
      "A3C episode 52, steps 426, time 0.4s\n",
      "A3C episode 53, steps 393, time 0.4s\n",
      "A3C episode 54, steps 80, time 0.1s\n",
      "A3C timestep 20000/20000, time 16.4s\n"
     ]
    }
   ],
   "source": [
    "a3c_agent = A3C(env)\n",
    "\n",
    "print(\"Starting A3C training\")\n",
    "a3c_agent.train(num_timesteps=20000)\n",
    "torch.save(a3c_agent.net.state_dict(), \"a3c_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8fdb9d4-b0ff-43d8-9b91-6377c1a4c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AlphaZero training\n",
      "AlphaZero episode 1/150, steps 199, time 1.6s\n",
      "AlphaZero episode 2/150, steps 485, time 5.2s\n",
      "AlphaZero episode 3/150, steps 49, time 0.4s\n",
      "AlphaZero episode 4/150, steps 479, time 5.2s\n",
      "AlphaZero episode 5/150, steps 415, time 4.3s\n",
      "AlphaZero episode 6/150, steps 479, time 5.1s\n",
      "AlphaZero episode 7/150, steps 463, time 4.8s\n",
      "AlphaZero episode 8/150, steps 495, time 5.5s\n",
      "AlphaZero episode 9/150, steps 441, time 4.8s\n",
      "AlphaZero episode 10/150, steps 276, time 2.7s\n",
      "AlphaZero episode 11/150, steps 598, time 7.5s\n",
      "AlphaZero episode 12/150, steps 376, time 4.0s\n",
      "AlphaZero episode 13/150, steps 362, time 3.8s\n",
      "AlphaZero episode 14/150, steps 381, time 3.9s\n",
      "AlphaZero episode 15/150, steps 281, time 2.6s\n",
      "AlphaZero episode 16/150, steps 266, time 2.6s\n",
      "AlphaZero episode 17/150, steps 110, time 0.9s\n",
      "AlphaZero episode 18/150, steps 538, time 6.2s\n",
      "AlphaZero episode 19/150, steps 324, time 3.5s\n",
      "AlphaZero episode 20/150, steps 590, time 7.1s\n",
      "AlphaZero episode 21/150, steps 501, time 6.1s\n",
      "AlphaZero episode 22/150, steps 471, time 4.7s\n",
      "AlphaZero episode 23/150, steps 404, time 3.9s\n",
      "AlphaZero episode 24/150, steps 612, time 7.7s\n",
      "AlphaZero episode 25/150, steps 374, time 3.8s\n",
      "AlphaZero episode 26/150, steps 576, time 7.0s\n",
      "AlphaZero episode 27/150, steps 461, time 5.1s\n",
      "AlphaZero episode 28/150, steps 339, time 3.5s\n",
      "AlphaZero episode 29/150, steps 419, time 4.5s\n",
      "AlphaZero episode 30/150, steps 699, time 9.1s\n",
      "AlphaZero episode 31/150, steps 472, time 5.4s\n",
      "AlphaZero episode 32/150, steps 618, time 7.8s\n",
      "AlphaZero episode 33/150, steps 608, time 7.8s\n",
      "AlphaZero episode 34/150, steps 324, time 3.3s\n",
      "AlphaZero episode 35/150, steps 299, time 2.9s\n",
      "AlphaZero episode 36/150, steps 406, time 4.4s\n",
      "AlphaZero episode 37/150, steps 442, time 4.3s\n",
      "AlphaZero episode 38/150, steps 446, time 4.9s\n",
      "AlphaZero episode 39/150, steps 152, time 1.3s\n",
      "AlphaZero episode 40/150, steps 241, time 2.3s\n",
      "AlphaZero episode 41/150, steps 377, time 4.1s\n",
      "AlphaZero episode 42/150, steps 552, time 6.5s\n",
      "AlphaZero episode 43/150, steps 443, time 5.0s\n",
      "AlphaZero episode 44/150, steps 401, time 4.3s\n",
      "AlphaZero episode 45/150, steps 447, time 4.9s\n",
      "AlphaZero episode 46/150, steps 507, time 5.9s\n",
      "AlphaZero episode 47/150, steps 526, time 6.3s\n",
      "AlphaZero episode 48/150, steps 410, time 4.5s\n",
      "AlphaZero episode 49/150, steps 411, time 4.6s\n",
      "AlphaZero episode 50/150, steps 408, time 4.5s\n",
      "AlphaZero episode 51/150, steps 416, time 4.5s\n",
      "AlphaZero episode 52/150, steps 403, time 4.3s\n",
      "AlphaZero episode 53/150, steps 387, time 4.3s\n",
      "AlphaZero episode 54/150, steps 329, time 3.5s\n",
      "AlphaZero episode 55/150, steps 240, time 2.3s\n",
      "AlphaZero episode 56/150, steps 423, time 4.5s\n",
      "AlphaZero episode 57/150, steps 39, time 0.3s\n",
      "AlphaZero episode 58/150, steps 485, time 5.6s\n",
      "AlphaZero episode 59/150, steps 418, time 4.6s\n",
      "AlphaZero episode 60/150, steps 389, time 4.1s\n",
      "AlphaZero episode 61/150, steps 350, time 3.5s\n",
      "AlphaZero episode 62/150, steps 601, time 7.8s\n",
      "AlphaZero episode 63/150, steps 367, time 3.8s\n",
      "AlphaZero episode 64/150, steps 354, time 3.8s\n",
      "AlphaZero episode 65/150, steps 511, time 6.1s\n",
      "AlphaZero episode 66/150, steps 452, time 5.1s\n",
      "AlphaZero episode 67/150, steps 291, time 2.7s\n",
      "AlphaZero episode 68/150, steps 350, time 3.6s\n",
      "AlphaZero episode 69/150, steps 184, time 1.7s\n",
      "AlphaZero episode 70/150, steps 190, time 1.8s\n",
      "AlphaZero episode 71/150, steps 410, time 4.2s\n",
      "AlphaZero episode 72/150, steps 347, time 3.5s\n",
      "AlphaZero episode 73/150, steps 333, time 3.3s\n",
      "AlphaZero episode 74/150, steps 562, time 6.9s\n",
      "AlphaZero episode 75/150, steps 609, time 7.6s\n",
      "AlphaZero episode 76/150, steps 401, time 4.4s\n",
      "AlphaZero episode 77/150, steps 362, time 3.7s\n",
      "AlphaZero episode 78/150, steps 426, time 4.7s\n",
      "AlphaZero episode 79/150, steps 333, time 3.4s\n",
      "AlphaZero episode 80/150, steps 439, time 5.0s\n",
      "AlphaZero episode 81/150, steps 501, time 5.7s\n",
      "AlphaZero episode 82/150, steps 180, time 1.6s\n",
      "AlphaZero episode 83/150, steps 632, time 8.2s\n",
      "AlphaZero episode 84/150, steps 311, time 3.1s\n",
      "AlphaZero episode 85/150, steps 393, time 3.9s\n",
      "AlphaZero episode 86/150, steps 528, time 6.1s\n",
      "AlphaZero episode 87/150, steps 335, time 3.4s\n",
      "AlphaZero episode 88/150, steps 332, time 3.4s\n",
      "AlphaZero episode 89/150, steps 356, time 3.7s\n",
      "AlphaZero episode 90/150, steps 457, time 5.2s\n",
      "AlphaZero episode 91/150, steps 417, time 4.6s\n",
      "AlphaZero episode 92/150, steps 185, time 1.8s\n",
      "AlphaZero episode 93/150, steps 392, time 4.2s\n",
      "AlphaZero episode 94/150, steps 301, time 2.7s\n",
      "AlphaZero episode 95/150, steps 498, time 5.6s\n",
      "AlphaZero episode 96/150, steps 458, time 5.3s\n",
      "AlphaZero episode 97/150, steps 357, time 3.2s\n",
      "AlphaZero episode 98/150, steps 371, time 3.7s\n",
      "AlphaZero episode 99/150, steps 526, time 6.5s\n",
      "AlphaZero episode 100/150, steps 348, time 3.4s\n",
      "AlphaZero episode 101/150, steps 405, time 4.0s\n",
      "AlphaZero episode 102/150, steps 176, time 1.7s\n",
      "AlphaZero episode 103/150, steps 343, time 3.5s\n",
      "AlphaZero episode 104/150, steps 534, time 5.8s\n",
      "AlphaZero episode 105/150, steps 432, time 4.8s\n",
      "AlphaZero episode 106/150, steps 258, time 2.0s\n",
      "AlphaZero episode 107/150, steps 291, time 2.6s\n",
      "AlphaZero episode 108/150, steps 515, time 5.6s\n",
      "AlphaZero episode 109/150, steps 498, time 5.5s\n",
      "AlphaZero episode 110/150, steps 140, time 1.2s\n",
      "AlphaZero episode 111/150, steps 271, time 2.5s\n",
      "AlphaZero episode 112/150, steps 544, time 6.4s\n",
      "AlphaZero episode 113/150, steps 427, time 4.5s\n",
      "AlphaZero episode 114/150, steps 294, time 2.9s\n",
      "AlphaZero episode 115/150, steps 282, time 2.6s\n",
      "AlphaZero episode 116/150, steps 373, time 3.8s\n",
      "AlphaZero episode 117/150, steps 634, time 7.8s\n",
      "AlphaZero episode 118/150, steps 58, time 0.5s\n",
      "AlphaZero episode 119/150, steps 293, time 2.7s\n",
      "AlphaZero episode 120/150, steps 340, time 3.3s\n",
      "AlphaZero episode 121/150, steps 326, time 3.1s\n",
      "AlphaZero episode 122/150, steps 370, time 3.8s\n",
      "AlphaZero episode 123/150, steps 603, time 7.4s\n",
      "AlphaZero episode 124/150, steps 392, time 4.1s\n",
      "AlphaZero episode 125/150, steps 327, time 3.2s\n",
      "AlphaZero episode 126/150, steps 228, time 2.0s\n",
      "AlphaZero episode 127/150, steps 78, time 0.6s\n",
      "AlphaZero episode 128/150, steps 560, time 6.7s\n",
      "AlphaZero episode 129/150, steps 634, time 8.0s\n",
      "AlphaZero episode 130/150, steps 442, time 4.7s\n",
      "AlphaZero episode 131/150, steps 473, time 5.2s\n",
      "AlphaZero episode 132/150, steps 170, time 1.6s\n",
      "AlphaZero episode 133/150, steps 177, time 1.4s\n",
      "AlphaZero episode 134/150, steps 67, time 0.5s\n",
      "AlphaZero episode 135/150, steps 362, time 3.6s\n",
      "AlphaZero episode 136/150, steps 345, time 3.3s\n",
      "AlphaZero episode 137/150, steps 187, time 1.7s\n",
      "AlphaZero episode 138/150, steps 547, time 6.3s\n",
      "AlphaZero episode 139/150, steps 218, time 2.0s\n",
      "AlphaZero episode 140/150, steps 161, time 1.4s\n",
      "AlphaZero episode 141/150, steps 526, time 6.2s\n",
      "AlphaZero episode 142/150, steps 400, time 4.2s\n",
      "AlphaZero episode 143/150, steps 272, time 2.5s\n",
      "AlphaZero episode 144/150, steps 610, time 7.4s\n",
      "AlphaZero episode 145/150, steps 538, time 6.2s\n",
      "AlphaZero episode 146/150, steps 416, time 4.4s\n",
      "AlphaZero episode 147/150, steps 226, time 2.0s\n",
      "AlphaZero episode 148/150, steps 262, time 2.4s\n",
      "AlphaZero episode 149/150, steps 624, time 6.7s\n",
      "AlphaZero episode 150/150, steps 318, time 2.8s\n",
      "AlphaZero total time 631.1s\n"
     ]
    }
   ],
   "source": [
    "alpha_zero_agent = AlphaZero(env)\n",
    "\n",
    "print(\"Starting AlphaZero training\")\n",
    "alpha_zero_agent.train(num_episodes=150)\n",
    "torch.save(alpha_zero_agent.net.state_dict(), \"alpha_zero_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8987e-5724-42bb-ac97-c1ad99e62b6c",
   "metadata": {},
   "source": [
    "# Gameplay loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60afa6ba-c662-435e-8f83-229c9d12b71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb9bf62d09a447ca8c530c19e8f65e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Agent:', index=1, options=('DQN', 'PPO', 'A3C', 'AlphaZero'), value='PPO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading trained models\n",
    "dqn_net = ChessNet().to(device)\n",
    "dqn_net.load_state_dict(torch.load(\"dqn_model.pth\"))\n",
    "ppo_net = ChessNet().to(device)\n",
    "ppo_net.load_state_dict(torch.load(\"ppo_model.pth\"))\n",
    "a3c_net = ChessNet().to(device)\n",
    "a3c_net.load_state_dict(torch.load(\"a3c_model.pth\"))\n",
    "alpha_zero_net = ChessNet().to(device)\n",
    "alpha_zero_net.load_state_dict(torch.load(\"alpha_zero_model.pth\"))\n",
    "\n",
    "# Mapping agents\n",
    "agents = {\n",
    "    \"DQN\": dqn_agent,\n",
    "    \"PPO\": ppo_agent,\n",
    "    \"A3C\": a3c_agent,\n",
    "    \"AlphaZero\": alpha_zero_agent\n",
    "}\n",
    "\n",
    "# Widget setup\n",
    "agent_dropdown = widgets.Dropdown(\n",
    "    options=[\"DQN\", \"PPO\", \"A3C\", \"AlphaZero\"],\n",
    "    value=\"PPO\",\n",
    "    description=\"Agent:\"\n",
    ")\n",
    "\n",
    "color_dropdown = widgets.Dropdown(\n",
    "    options=[(\"White\", chess.WHITE), (\"Black\", chess.BLACK)],\n",
    "    value=chess.WHITE,\n",
    "    description=\"Your Color:\"\n",
    ")\n",
    "\n",
    "start_button = widgets.Button(\n",
    "    description=\"Start Game\",\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Click to start the game\"\n",
    ")\n",
    "\n",
    "move_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description=\"Your Move:\",\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Submit Move\",\n",
    "    button_style=\"info\",\n",
    "    tooltip=\"Submit your move\",\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "abort_button = widgets.Button(\n",
    "    description=\"Abort Game\",\n",
    "    button_style=\"danger\",\n",
    "    tooltip=\"Stop the current game\",\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "last_move_label = widgets.Label(\n",
    "    value=\"Last move: None\",\n",
    "    layout={'width': '200px'}\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# Game state\n",
    "game_state = {\n",
    "    \"board\": None,\n",
    "    \"env\": None,\n",
    "    \"agent\": None,\n",
    "    \"human_color\": None,\n",
    "    \"obs\": None,\n",
    "    \"move_count\": 0,\n",
    "    \"active\": False,\n",
    "    \"last_move\": None\n",
    "}\n",
    "\n",
    "# Update board display\n",
    "def update_board():\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if game_state[\"board\"]:\n",
    "            display(SVG(chess.svg.board(game_state[\"board\"], size=400)))\n",
    "            print(f\"Move {game_state['move_count']}: Turn = {'White' if game_state['board'].turn == chess.WHITE else 'Black'}\")\n",
    "            print(f\"Last move: {game_state['last_move'] if game_state['last_move'] else 'None'}\")\n",
    "            last_move_label.value = f\"Last move: {game_state['last_move'] if game_state['last_move'] else 'None'}\"\n",
    "            if game_state[\"board\"].is_game_over():\n",
    "                print(f\"Game over. Result: {game_state['board'].result()}\")\n",
    "\n",
    "def update_move_dropdown():\n",
    "    if game_state[\"board\"] and game_state[\"board\"].turn == game_state[\"human_color\"] and game_state[\"active\"]:\n",
    "        legal_moves = list(game_state[\"board\"].legal_moves)\n",
    "        move_dropdown.options = [move.uci() for move in legal_moves]\n",
    "        move_dropdown.disabled = False\n",
    "        submit_button.disabled = False\n",
    "        abort_button.disabled = False\n",
    "    else:\n",
    "        move_dropdown.options = []\n",
    "        move_dropdown.disabled = True\n",
    "        submit_button.disabled = True\n",
    "        abort_button.disabled = not game_state[\"active\"]\n",
    "\n",
    "def start_game(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        agent_name = agent_dropdown.value\n",
    "        if agent_name not in agents:\n",
    "            print(f\"No agent named {agent_name}\")\n",
    "            return\n",
    "        \n",
    "        game_state[\"agent\"] = agents[agent_name]\n",
    "        game_state[\"human_color\"] = color_dropdown.value\n",
    "        game_state[\"board\"] = chess.Board()\n",
    "        game_state[\"env\"] = ChessEnv()\n",
    "        game_state[\"obs\"], _ = game_state[\"env\"].reset()\n",
    "        game_state[\"board\"] = game_state[\"env\"].board.copy()\n",
    "        game_state[\"move_count\"] = 0\n",
    "        game_state[\"active\"] = True\n",
    "        game_state[\"last_move\"] = None\n",
    "\n",
    "        print(f\"Game started against {agent_name}. You are {'White' if game_state['human_color'] == chess.WHITE else 'Black'}.\")\n",
    "        update_board()\n",
    "        update_move_dropdown()\n",
    "        if game_state[\"board\"].turn != game_state[\"human_color\"]:\n",
    "            ai_move()\n",
    "\n",
    "def ai_move():\n",
    "    if not game_state[\"active\"] or game_state[\"board\"].is_game_over():\n",
    "        return\n",
    "    \n",
    "    agent_name = agent_dropdown.value\n",
    "    if agent_name == \"AlphaZero\":\n",
    "        move = game_state[\"agent\"].mcts(game_state[\"board\"])\n",
    "        if move is None:\n",
    "            with output:\n",
    "                print(\"AI failed to select a move\")\n",
    "            game_state[\"active\"] = False\n",
    "            update_move_dropdown()\n",
    "            return\n",
    "        action_idx = list(game_state[\"env\"].board.legal_moves).index(move)\n",
    "    else:\n",
    "        state_tensor = torch.tensor(game_state[\"obs\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            if agent_name == \"DQN\":\n",
    "                q_values = dqn_net(state_tensor)\n",
    "                action_idx = q_values.argmax().item()\n",
    "            else:  # PPO, A3C\n",
    "                logits, _ = (ppo_net if agent_name == \"PPO\" else a3c_net)(state_tensor, return_value=True)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                action_idx = torch.multinomial(probs, 1).item()\n",
    "        move = game_state[\"env\"]._index_to_move(action_idx, list(game_state[\"board\"].legal_moves))\n",
    "\n",
    "    game_state[\"obs\"], reward, terminated, truncated, _ = game_state[\"env\"].step(action_idx)\n",
    "    game_state[\"board\"] = game_state[\"env\"].board.copy()\n",
    "    game_state[\"move_count\"] += 1\n",
    "    game_state[\"last_move\"] = move.uci()\n",
    "\n",
    "    with output:\n",
    "        print(f\"{agent_name}'s move: {move.uci()}, Reward={reward}, Terminated={terminated}, Truncated={truncated}\")\n",
    "        if terminated or truncated:\n",
    "            print(\"Game terminated after AI move\")\n",
    "            game_state[\"active\"] = False\n",
    "    \n",
    "    update_board()\n",
    "    update_move_dropdown()\n",
    "\n",
    "def submit_move(b):\n",
    "    if not game_state[\"active\"] or game_state[\"board\"].is_game_over():\n",
    "        return\n",
    "    \n",
    "    move_uci = move_dropdown.value\n",
    "    try:\n",
    "        move = chess.Move.from_uci(move_uci)\n",
    "        legal_moves = list(game_state[\"board\"].legal_moves)\n",
    "        if move in legal_moves:\n",
    "            game_state[\"board\"].push(move)\n",
    "            env_legal_moves = list(game_state[\"env\"].board.legal_moves)\n",
    "            action_idx = env_legal_moves.index(move)\n",
    "            game_state[\"obs\"], reward, terminated, truncated, _ = game_state[\"env\"].step(action_idx)\n",
    "            game_state[\"board\"] = game_state[\"env\"].board.copy()\n",
    "            game_state[\"move_count\"] += 1\n",
    "            game_state[\"last_move\"] = move.uci()\n",
    "\n",
    "            with output:\n",
    "                print(f\"Your move: {move.uci()}, Reward={reward}, Terminated={terminated}, Truncated={truncated}\")\n",
    "                if terminated or truncated:\n",
    "                    print(\"Game terminated after your move\")\n",
    "                    game_state[\"active\"] = False\n",
    "            \n",
    "            update_board()\n",
    "            update_move_dropdown()\n",
    "            if not (terminated or truncated) and game_state[\"board\"].turn != game_state[\"human_color\"]:\n",
    "                ai_move()\n",
    "        else:\n",
    "            with output:\n",
    "                print(\"Illegal move. Try again.\")\n",
    "    except ValueError as e:\n",
    "        with output:\n",
    "            print(f\"Invalid UCI format: {e}. Try again.\")\n",
    "\n",
    "def abort_game(b):\n",
    "    if game_state[\"active\"]:\n",
    "        game_state[\"active\"] = False\n",
    "        game_state[\"board\"] = None\n",
    "        game_state[\"env\"] = None\n",
    "        game_state[\"agent\"] = None\n",
    "        game_state[\"human_color\"] = None\n",
    "        game_state[\"obs\"] = None\n",
    "        game_state[\"move_count\"] = 0\n",
    "        game_state[\"last_move\"] = None\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(\"Game aborted.\")\n",
    "        update_move_dropdown()\n",
    "        last_move_label.value = \"Last move: None\"\n",
    "\n",
    "# Bind buttons\n",
    "start_button.on_click(start_game)\n",
    "submit_button.on_click(submit_move)\n",
    "abort_button.on_click(abort_game)\n",
    "\n",
    "# Display UI\n",
    "display(widgets.VBox([agent_dropdown, color_dropdown, start_button, move_dropdown, submit_button, abort_button, last_move_label, output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3cb3e-53ba-4746-8549-5bdf638c0089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe620f-53c2-4d16-9452-cd59e12b9a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00881c-adee-4ce7-947f-0ee798b2c113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898f637-2959-4a7d-b8d4-a462a19fa0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
